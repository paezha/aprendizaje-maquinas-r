---
title: "Artificial Neural Networks - Session 10: Self Organizing Maps III"
---

# Introduction

Previously we used a toy example to illustrate the application of self-organizing maps. In this session, we will now use a real-world example to develop a customer segmentation system.

This example uses data from Ireland used by Shane Lynn in this [blogpost](https://www.shanelynn.ie/self-organising-maps-for-customer-segmentation-using-r/). The data are sourced from the Irish Census of 2011 and includes the Dublin area. For this session, we will continue working with the package `SOMbrero`.

As usual, we will begin by clearing the workspace:
```{r}
rm(list = ls())
```

And also loading the packages used for this session:
```{r message = FALSE}
library(AppliedPredictiveModeling) # Functions and Data Sets for 'Applied Predictive Modeling'
library(caret) # Classification and Regression Training
library(formattable) # Create 'Formattable' Data Structures
library(kableExtra) # Construct Complex Table with 'kable' and Pipe Syntax
library(knitr) # A General-Purpose Package for Dynamic Report Generation in R
library(plotly) # Create Interactive Web Graphics via 'plotly.js'
library(rpart) # Recursive Partitioning and Regression Trees
library(rpart.plot) # Plot 'rpart' Models: An Enhanced Version of 'plot.rpart'
library(sf) # Simple Features for R
library(spdep) # Spatial Dependence: Weighting Schemes, Statistics
library(SOMbrero) # SOM Bound to Realize Euclidean and Relational Outputs
library(tidyverse) # Easily Install and Load the 'Tidyverse'
library(tmap) # Thematic Maps
```

# Load and preprocess example data

```{r}
data <- read.csv("data/Selected Themes.csv")
```

{Using the function defined above process the data and print a summary:
```{r}
summary(data)
```



Geospatial data:
```{r}
ireland_map <- st_read("data/Census2011_Small_Areas_generalised20m.shp")

# Join the tabular data
ireland_map <- ireland_map |>
  left_join(data,
            by = "SMALL_AREA") |>
  st_as_sf()

ggplot() +
  geom_sf(data = ireland_map)
```

Extract only the counties corresponding to the Dublin area:
```{r}
ireland_map <- ireland_map |>
  filter(COUNTYNAME %in% c("Fingal", "Dublin City", "South Dublin", "Dn Laoghaire-Rathdown"))
```

This is the map of the small areas in the Dublin area: 
```{r}
ggplot() +
  geom_sf(data = ireland_map)
```

It might be useful to visualize some of these variables. The following code creates a `tmap` object:, essentially a choropleth map of the variable specified:
```{r fig.height= 8}
map <- tm_shape(ireland_map) +
  tm_polygons("internet_percent")
```

The map can be displayed statically, or dynamically, depending on the `tmap_mode` selected. For static maps use "plot" and for dynamic maps use "view":
```{r plot-map, message=FALSE}
tmap_mode("view")
map
```

# Train SOM

## Simple example with only two variables

To illustrate SOMs in two dimensions, we will use only two of the variables in the dataframe, `avr_age` and `avr_education_level`:
```{r}
df <- ireland_map |>
  st_drop_geometry() |>
  select(avr_age,
             avr_education_level)
```

```{r}
ggplot(data = df, 
       aes(x = avr_age, 
           y = avr_education_level)) + 
  geom_point()
```


Given these two attributes, a SOM can be trained using the function `trainSOM` from `SOMbrero`. Only one argument is needed, the input data `x.data`. Other arguments can be assigned by default. In what follows the dimension of the map is selected in such a way that the number of cells is $5\sqrt n$, and I request that 100 intermediate steps be saved. 
```{r cache = TRUE}
set.seed(593)
# set size of SOM
nrows <- 17
ncols <- 17
# initialize
#initSOM(maxit = 5000)
# run the SOM algorithm 
som1 <- trainSOM(x.data = df, 
                 dimension=c(nrows, ncols), 
                 nb.save = 25, radius.type = "letremy", maxit = 5000, verbose = TRUE)
```

Remember that the `somRes` object includes the following items: 

- clustering: the results of the clustering algorithm, which gives each observation a number that corresponds to their best matching unit (BMU).

- prototypes: the final weights of the cells.

- energy: the final energy.

- backup: a list that includes the prototypes (the weights) at intermediate steps, the steps at which backups were recorded, intermediate clustering results, etc.

Since backups were requested, it is possible to plot the evolution of the training algorithm. For instance, the following plot shows the energy:
```{r}
plot(som1, what = "energy")
```

Which can be visualized by means of a _hitmap_:
```{r}
plot(som1, what = "obs", type = "hitmap")
```

```{r}
plot(som1, what = "prototypes", type = "mds")
```


```{r}
som.grid <- cell2nb(nrow = nrows, ncol = ncols, type = "rook")
```

```{r}
som.grid.final <- data.frame(x1 = numeric(length(unlist(som.grid))))
count <- 0
for(i in 1:(nrows * ncols)){
  for(j in 1:length(som.grid[[i]])){
    count <- count + 1
    som.grid.final$x1[count] <- som1$prototypes[i,1]
    som.grid.final$y1[count] <- som1$prototypes[i,2]
    som.grid.final$x2[count] <- som1$prototypes[som.grid[[i]][j],1]
    som.grid.final$y2[count] <- som1$prototypes[som.grid[[i]][j],2]
  }
}  
```


```{r}
ggplot(data = data.frame(node = 1:ncols * nrows, som1$prototypes), 
       aes(x = avr_age, y = avr_education_level)) +
  geom_segment(data = som.grid.final, 
               aes(x = x1, y = y1, xend = x2, yend = y2)) +
  geom_point(aes(color = node), size = 4) +
  geom_point(shape = 1, size = 4) +
  scale_color_distiller(palette = "RdYlBu")
```

## More attributes

Lets now try this but with the full array of numeric stats:
```{r}
df <- ireland_map |>
  st_drop_geometry() |>
  select(avr_age:widow_percent)
```

A rule of thumb for selecting the size of the SOM is $5\sqrt n$ neurons. Since the size of the sample is `r nrow(data)`, this means a square with `r round(sqrt(5 * sqrt(nrow(data))))`:
```{r}
set.seed(593)
# run the SOM algorithm 
nrows <- 17
ncols <- 17
som2 <- trainSOM(x.data = df, dimension=c(nrows, ncols), radius.type = "letremy", verbose = TRUE)
```

This gives the summary for the map:
```{r}
summary(som2)
```

The results of ANOVA indicate that all variables contribute to discrimination in the clustering.

The following shows the number of cases assigned to each node:
```{r}
table(som2$clustering)
```

Which can be visualized by means of a _hitmap_:
```{r}
plot(som2, what = "obs", type = "hitmap")
```

The hitmap does not reveal empty cells, that is, neurons that did not classify a single observation.

# tools for exploring the results

Interpretation of clusters typically involves descriptive statistics of the clusters. `SOMbrero` implements a number of graphical tools to support interpretation.

The first example is a heatmap of the variables in clusters:
```{r fig.width= 10}
plot(som2, what="obs", type="color", variable=1, print.title=TRUE, 
     main="avg_age")
```

# Create super clusters

The clusters obtained from the SOM are perhaps too numerous to be of practical use. It is then convenient to further group them to obtain super clusters. This is easily done in `SOMbrero` by means of the function `superClass`, which takes the results of `trainSOM` and applies a hierarchical clustering algorithm to reduce the number of classes.

In this example, I select $k=8$ for the number of super clusters:
```{r}
som2.sc <- superClass(som2, k=8)
summary(som2.sc)
```
It can be seen that all variables retain their significance. The frequency table, on the other hand, shows how many of the original clusters have been assigned to each of the super clusters.

The hierarchical clustering results can be visualized as follows:
```{r fig.width=12}
plot(som2.sc, plot.var=FALSE)
```

And also plotted in the grid.
```{r}
plot(som2.sc, type="grid", plot.legend=TRUE)
```

# Profiling the results

Once that clusters have been obtained, it is interesting to profile the results. By profiling the results, we can derive insights as to the (hopefully) distinctive characteristics of the clusters. For instance, is Cluster 1 mostly composed of young people? People with internet access? 

Explore the results. First append the results of the cluster and super cluster analysis to dataframe:
```{r}
ireland_map$cluster.som2 <- som2$clustering
ireland_map$scluster.som2 <- som2.sc$cluster[som2$clustering]
```

Select four of the features with the greatest discriminatory power, and do a feature plot (from package `caret`):
```{r fig.width=16}
transparentTheme(trans = .4)
featurePlot(x = data.frame(ireland_map$avr_education_level, 
                           ireland_map$avr_num_cars, 
                           ireland_map$rented_percent, 
                           ireland_map$internet_percent), 
            y = as.factor(ireland_map$scluster.som2), 
            plot = "pairs",
            ## Add a key at the top
            auto.key = list(columns = 8))
```

Descriptive statistics of superclusters.

First group data by super cluster:
```{r}
super.clusters <- ireland_map |>
  group_by(scluster.som2)
```

Summarize profiles by super cluster:
```{r}
profiles <- summarize(super.clusters, 
                      Age = mean(avr_age), 
                      `HH Size` = mean(avr_household_size),
                      Cars = mean(avr_num_cars),
                      Health = mean(avr_health),
                      `Pct Rent` = mean(rented_percent),
                      `Pct Unemployed` = mean(unemployment_percent),
                      `Pct Internet` = mean(internet_percent),
                      Single = mean(single_percent),
                      Married = mean(married_percent),
                      Separated = mean(separated_percent),
                      Divorced = mean(divorced_percent),
                      Widow = mean(widow_percent))
profiles <- rename(profiles, `Super Cluster` = scluster.som2)
```

Format table and print:
```{r}
profiles %>%
  format.data.frame(digits = 3) %>%
  mutate(Age = cell_spec(Age, "html", color = ifelse(Age < mean(data$avr_age), "red", "blue")),
         `HH Size` = cell_spec(`HH Size`, "html", color = ifelse(`HH Size` < mean(data$avr_household_size), "red", "blue")),
         Cars = cell_spec(Cars, "html", color = ifelse(Cars < mean(data$avr_num_cars), "red", "blue")),
         Health = cell_spec(Health, "html", color = ifelse(Health < mean(data$avr_health), "red", "blue")),
         `Pct Rent` = cell_spec(`Pct Rent`, "html", color = ifelse(`Pct Rent` < mean(data$unemployment_percent), "red", "blue")),
         `Pct Unemployed` = color_bar("lightpink")(`Pct Unemployed`),
         `Pct Internet` = cell_spec(`Pct Internet`, "html", color = ifelse(`Pct Internet` < mean(data$internet_percent), "red", "blue")),
         Single = cell_spec(Single, "html", color = ifelse(Single < mean(data$single_percent), "red", "blue")),
         Married = cell_spec(Married, "html", color = ifelse(Married < mean(data$married_percent), "red", "blue")),
         Separated = cell_spec(Separated, "html", color = ifelse(Separated < mean(data$separated_percent), "red", "blue")),
         Divorced = cell_spec(Divorced, "html", color = ifelse(Divorced < mean(data$divorced_percent), "red", "blue")),
         Widow = cell_spec(Widow, "html", color = ifelse(Widow < mean(data$widow_percent), "red", "blue"))) %>%
  kable("html", escape = F, digits = 2) %>%
  column_spec(1:13, width = 16) %>%
  kable_styling("striped", full_width = F)
```

Map the superclusters:
```{r}
map2 <- tm_shape(ireland_map) +
  tm_polygons("scluster.som2")
map2
```

# Assessing the quality of the classifier

A thorny issue in unsupervised learning is how to assess the quality of a clustering algorithm. Say that you train a few different classifiers, either using different methods or with different combinations of variables. How do you decide which to use?

One possibility is to analyze the results of the clustering algorithm by means of a classification technique. This is illustrated here by means of a decision tree. A decision tree is a supervised learning technique, which in this case can use the results of the clustering algorithm as labels, and the input variables as features. If the clustering algorithm performed well, class membership must be relatively homogeneous. Since a decision tree 

Select data for classification:
```{r}
#df <- select(data, -c(SMALL_AREA, cluster.som2))
df <- ireland_map |>
  st_drop_geometry() |>
  transmute(`Super Cluster` = as.factor(scluster.som2),
                Age = avr_age,
                `HH Size` = avr_household_size,
                Cars = avr_num_cars,
                Health = avr_health,
                `Pct Rent` = rented_percent,
                `Pct Unemployed` = unemployment_percent,
                `Pct Internet` = internet_percent,
                Single = single_percent,
                Married = married_percent,
                Separated = separated_percent,
                Divorced = divorced_percent,
                Widow = widow_percent)
```

Train a decision tree using the data:
```{r}
classmod.som2 <- rpart(`Super Cluster` ~., data = df)
```

Plot results of the decision tree:
```{r fig.width= 12}
rpart.plot(classmod.som2, cex = 0.75, box.palette = 0, type = 3)
```

## Train a SOM using only the variables identified by the Decision Tree

Lets now try this but with the full array of numeric stats:
```{r}
df <- ireland_map |>
  st_drop_geometry() |>
    transmute(Age = avr_age,
                Cars = avr_num_cars,
                Health = avr_health,
                `Pct Rent` = rented_percent,
                `Pct Unemployed` = unemployment_percent,
                `Pct Internet` = internet_percent,
                Separated = separated_percent)
```

```{r}
set.seed(593)
# run the SOM algorithm 
nrows <- 17
ncols <- 17
som3 <- trainSOM(x.data = df, dimension=c(nrows, ncols), radius.type = "letremy", verbose = TRUE)
```

This gives the summary for the map:
```{r}
summary(som3)
```


The clusters obtained from the SOM are perhaps too numerous to be of practical use. It is then convenient to further group them to obtain super clusters. This is easily done in `SOMbrero` by means of the function `superClass`, which takes the results of `trainSOM` and applies a hierarchical clustering algorithm to reduce the number of classes.

In this example, I select $k=8$ for the number of super clusters:
```{r}
som3.sc <- superClass(som3, k=8)
summary(som3.sc)
```
It can be seen that all variables retain their significance. The frequency table, on the other hand, shows how many of the original clusters have been assigned to each of the super clusters.

The clustering results can be visualized as follows
```{r}
plot(som3.sc, type="grid", plot.legend=TRUE)
```

Explore the results. First append the results of the cluster and super cluster analysis to dataframe:
```{r}
ireland_map$cluster.som3 <- som3$clustering
ireland_map$scluster.som3 <- som3.sc$cluster[som3$clustering]
```

```{r}
#df <- select(data, -c(SMALL_AREA, cluster.som2))
df <- ireland_map |>
  st_drop_geometry() |>
  transmute(`Super Cluster` = as.factor(scluster.som2),
                Age = avr_age,
                Cars = avr_num_cars,
                Health = avr_health,
                `Pct Rent` = rented_percent,
                `Pct Unemployed` = unemployment_percent,
                `Pct Internet` = internet_percent,
                Separated = separated_percent)
```

Train a decision tree using the data:
```{r}
classmod.som3 <- rpart(`Super Cluster` ~., data = df)
```

Plot results of the decision tree:
```{r fig.width= 12}
rpart.plot(classmod.som3, cex = 0.75, box.palette = 0, type = 3)
```

# Alternative table format

Format table and print:
```{r}
profiles %>%
  format.data.frame(digits = 3) %>%
  mutate(Age = cell_spec(Age, "html", color = ifelse(Age < mean(data$avr_age), "red", "blue")),
         `HH Size` = cell_spec(`HH Size`, "html", color = ifelse(`HH Size` < mean(data$avr_household_size), "red", "blue")),
         Cars = cell_spec(Cars, "html", color = ifelse(Cars < mean(data$avr_num_cars), "red", "blue")),
         Health = cell_spec(Health, "html", color = ifelse(Health < mean(data$avr_health), "red", "blue")),
         `Pct Rent` = cell_spec(`Pct Rent`, "html", color = ifelse(`Pct Rent` < mean(data$unemployment_percent), "red", "blue")),
         `Pct Unemployed` = cell_spec(`Pct Unemployed`, "html", color = ifelse(`Pct Unemployed` < mean(data$unemployment_percent), "red", "blue")),
         `Pct Internet` = cell_spec(`Pct Internet`, "html", color = ifelse(`Pct Internet` < mean(data$internet_percent), "red", "blue")),
         Single = cell_spec(Single, "html", color = ifelse(Single < mean(data$single_percent), "red", "blue")),
         Married = cell_spec(Married, "html", color = ifelse(Married < mean(data$married_percent), "red", "blue")),
         Separated = cell_spec(Separated, "html", color = ifelse(Separated < mean(data$separated_percent), "red", "blue")),
         Divorced = cell_spec(Divorced, "html", color = ifelse(Divorced < mean(data$divorced_percent), "red", "blue")),
         Widow = cell_spec(Widow, "html", color = ifelse(Widow < mean(data$widow_percent), "red", "blue"))) %>%
  kable("html", escape = F, digits = 2) %>%
  kable_styling("striped", full_width = F)
```



